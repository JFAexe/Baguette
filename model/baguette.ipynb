{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XVtjOXWbyGkE"
      },
      "source": [
        "# Init"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "i3vLo1GJjAhP"
      },
      "outputs": [],
      "source": [
        "from os import environ as ENV\n",
        "from uuid import uuid4\n",
        "from datetime import date\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from google.colab import userdata, drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XydIJCF1BHt8"
      },
      "outputs": [],
      "source": [
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "mD62fPhsR17U"
      },
      "outputs": [],
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "torch.set_default_device(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0oc8Eum-AbHM"
      },
      "outputs": [],
      "source": [
        "np.random.seed(42)\n",
        "torch.manual_seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "TKGCmCDE6eJr"
      },
      "outputs": [],
      "source": [
        "data_path  = 'drive/MyDrive/ml/data/baguettes_clean.tsv'\n",
        "model_path = f'drive/MyDrive/ml/models/baguette'\n",
        "\n",
        "iteration_step = 1\n",
        "\n",
        "train_data_path = f'drive/MyDrive/ml/data/baguettes_train-{iteration_step}.txt'\n",
        "valid_data_path = f'drive/MyDrive/ml/data/baguettes_valid-{iteration_step}.txt'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "nYJCHvc8JTJ9"
      },
      "outputs": [],
      "source": [
        "ENV['HF_HOME']        = '/root/hf_home'\n",
        "ENV['HF_TOKEN']       = userdata.get('HF_TOKEN')\n",
        "ENV['MODEL_PATH']     = model_path\n",
        "ENV['TRAIN_PATH']     = train_data_path\n",
        "ENV['VALID_PATH']     = valid_data_path\n",
        "ENV['WANDB_DISABLED'] = 'true'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "URc-V5UhB-zh"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "git clone --depth 1 https://github.com/huggingface/transformers\n",
        "cd transformers\n",
        "pip install ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zJySVMprCCG7"
      },
      "outputs": [],
      "source": [
        "!pip3 install datasets\n",
        "!pip3 install evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jt8byGfQCD4C"
      },
      "outputs": [],
      "source": [
        "!wget https://raw.githubusercontent.com/huggingface/transformers/main/examples/pytorch/language-modeling/run_clm.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NDPzzycz594U"
      },
      "source": [
        "# Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zAlDx4z280dI"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv(data_path, sep='\\t')\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UGwFSic5_zZc"
      },
      "outputs": [],
      "source": [
        "files_count = 1\n",
        "valid_size  = 0.4\n",
        "\n",
        "valid_ind = np.random.choice(data.shape[0], int(data.shape[0] * valid_size), replace=False)\n",
        "train_ind = [i for i in range(len(data)) if i not in valid_ind]\n",
        "\n",
        "valid_per_file = len(valid_ind) // files_count\n",
        "train_per_file = (data.shape[0] - len(valid_ind)) // files_count\n",
        "\n",
        "for i in range(1, files_count+1):\n",
        "  begin = valid_per_file * (i-1)\n",
        "  end   = valid_per_file * i\n",
        "\n",
        "  with open(f'baguettes_valid-{i}.txt', 'w') as file:\n",
        "    file.write('\\n'.join([data.iloc[id]['baguette'] for id in valid_ind[begin:end]]))\n",
        "\n",
        "for i in range(1, files_count+1):\n",
        "  begin = train_per_file * (i-1)\n",
        "  end   = train_per_file * i\n",
        "\n",
        "  with open(f'baguettes_train-{i}.txt', 'w') as file:\n",
        "    file.write('\\n'.join([data.iloc[id]['baguette'] for id in train_ind[begin:end]]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TH2dpFSOqmoN"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xp9ApiUADElb"
      },
      "outputs": [],
      "source": [
        "!python run_clm.py \\\n",
        "  --model_type=gpt2 \\\n",
        "  --model_name_or_path sberbank-ai/rugpt3small_based_on_gpt2 \\\n",
        "  --dataset_config_name plain_text \\\n",
        "  --do_train \\\n",
        "  --train_file=${TRAIN_PATH} \\\n",
        "  --per_device_train_batch_size 8 \\\n",
        "  --do_eval \\\n",
        "  --validation_file=${VALID_PATH} \\\n",
        "  --per_device_eval_batch_size 8 \\\n",
        "  --fp16 True \\\n",
        "  --block_size 2048 \\\n",
        "  --num_train_epochs 10 \\\n",
        "  --gradient_accumulation_steps 4 \\\n",
        "  --gradient_checkpointing True \\\n",
        "  --optim adafactor \\\n",
        "  --output_dir=${MODEL_PATH} \\\n",
        "  --overwrite_output_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kwxxbvgzqqXK"
      },
      "source": [
        "# Run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "XSppDiLIDG6V"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import re\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "bh9xOvZXx41b"
      },
      "outputs": [],
      "source": [
        "model_path = 'drive/MyDrive/ml/models/baguette'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "AWMpZ6pxqPJF"
      },
      "outputs": [],
      "source": [
        "TOKEN_BOS = '<BOS>'\n",
        "TOKEN_EOS = '<EOS>'\n",
        "TOKEN_PAD = '<PAD>'\n",
        "\n",
        "special_tokens_dict = {\n",
        "  'bos_token': TOKEN_BOS,\n",
        "  'eos_token': TOKEN_EOS,\n",
        "  'pad_token': TOKEN_PAD,\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y6aLDW0vDIWE",
        "outputId": "d3c77a67-9853-4023-b191-e8be41d93bdb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Embedding(50260, 768)"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer = GPT2Tokenizer.from_pretrained(model_path)\n",
        "tokenizer.add_special_tokens(special_tokens_dict)\n",
        "tokenizer.bos_token_id = tokenizer.convert_tokens_to_ids(TOKEN_BOS)\n",
        "tokenizer.eos_token_id = tokenizer.convert_tokens_to_ids(TOKEN_EOS)\n",
        "tokenizer.pad_token_id = tokenizer.convert_tokens_to_ids(TOKEN_PAD)\n",
        "\n",
        "model = GPT2LMHeadModel.from_pretrained(model_path).to(device)\n",
        "model.resize_token_embeddings(len(tokenizer))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "E7NSK9tGYNHg"
      },
      "outputs": [],
      "source": [
        "def gen_fragment(context, tokenizer, model, device, temperature=1.0, min_size=128, max_size=256):\n",
        "  input_ids = tokenizer.encode(context.upper(), add_special_tokens=False, return_tensors='pt').to(device)\n",
        "  input_ids = input_ids[:, -1024:]\n",
        "  input_size = input_ids.size(1)\n",
        "\n",
        "  output_sequences = model.generate(\n",
        "    input_ids=input_ids,\n",
        "    max_length=input_size + max_size,\n",
        "    min_length=input_size + min_size,\n",
        "    top_p=0.6,\n",
        "    top_k=100,\n",
        "    do_sample=True,\n",
        "    num_return_sequences=1,\n",
        "    temperature=temperature,\n",
        "    pad_token_id=tokenizer.pad_token_id,\n",
        "    eos_token_id=tokenizer.eos_token_id,\n",
        "    no_repeat_ngram_size=6\n",
        "  )\n",
        "\n",
        "  if len(output_sequences.shape) > 2:\n",
        "    output_sequences.squeeze_()\n",
        "\n",
        "  out = tokenizer.decode(output_sequences[0], clean_up_tokenization_spaces=True)\n",
        "  generated_text = out.replace(tokenizer.bos_token, '').replace(tokenizer.eos_token, '').replace(tokenizer.pad_token, '\\n@\\n')\n",
        "  generated_text = re.sub(r'[BEOSPAD<>|&%]+', '', re.sub(r'<+(\\w+)?>*', '', re.sub('\\n+', '', re.sub('(@\\s*@\\s*)', '@', generated_text))))\n",
        "  generated_text = generated_text if (pos := generated_text.rfind('@')) == -1 else generated_text[:pos]\n",
        "\n",
        "  return generated_text.replace(' @ ', '@').replace('@', '\\n@\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "e-o9eU-ZYcYs",
        "outputId": "05ad972e-ae97-41d6-f727-d90059ad78c0"
      },
      "outputs": [],
      "source": [
        "gen_fragment(\n",
        "  'Напиши багет',\n",
        "  tokenizer=tokenizer,\n",
        "  model=model,\n",
        "  device=device,\n",
        "  temperature=1.6,\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
